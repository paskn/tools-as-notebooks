{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Collocations Pair Tool\n",
    "\n",
    "**Purpose:** This notebook plots the log ratio of frequencies of words collocated with a user-specified pair of words (e.g., 'he' vs 'she'). It helps visualize which words are more likely to appear with one word over the other in a given text corpus.\n",
    "\n",
    "**Inputs:**\n",
    "* A CSV file with a column containing lemmatized text.\n",
    "* The name of the column with the lemmatized text.\n",
    "* Two words to compare (e.g., 'he', 'she').\n",
    "* Desired name for the output plot image and data CSV.\n",
    "\n",
    "**Outputs:**\n",
    "* A plot showing words more associated with one or the other target word.\n",
    "* A CSV file containing the data used for the plot (bigram counts, ratios).\n",
    "* Files are saved to Google Drive and offered for download.\n",
    "\n",
    "**Instructions:**\n",
    "* Run cells sequentially.\n",
    "* Upload CSV when prompted.\n",
    "* Configure parameters in the 'User Input Configuration' section.\n",
    "* Authenticate Google Drive if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using This Notebook & Viewing Code\n",
    "\n",
    "**Cell Visibility:** This Colab notebook uses `#@title` directives for its main operational cells (like \"Step 1: Install Libraries\", \"Step 2: Import Libraries\", etc.). This allows you to collapse or expand the code in these cells.\n",
    "*   To **hide the code** for a cell, click the small arrow next to its title or in the cell toolbar.\n",
    "*   To **show the code**, click the arrow again.\n",
    "This helps in focusing on the instructions and outputs rather than the underlying code, if you prefer.\n",
    "\n",
    "**Simplified Inputs:** For cells where you need to provide input (like \"Step 5: Configure Parameters\"), this notebook uses Colab's \"form\" feature. You can enter your parameters directly in the input fields provided, and you don't need to modify the code in that cell unless you want to change default behaviors.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: Install libraries required for the notebook.\n",
    "# This cell installs the necessary libraries for the notebook.\n",
    "# It will only run once or if the libraries are not already installed in your Colab environment.\n",
    "print(\"Installing pandas, nltk, matplotlib, and seaborn...\")\n",
    "!pip install pandas nltk matplotlib seaborn -q\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Import the libraries needed for the script.\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "import nltk\n",
    "from nltk.util import ngrams # <--- ADDED THIS LINE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Starting library imports and NLTK resource checks...\")\n",
    "\n",
    "# NLTK Resource Download Section\n",
    "nltk_resources = [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('tokenizers/punkt_tab/english/', 'punkt_tab') # Check for the specific English path directory\n",
    "]\n",
    "\n",
    "for resource_path, resource_id in nltk_resources:\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"NLTK resource '{resource_id}' (path: {resource_path}) already available.\")\n",
    "    except LookupError:\n",
    "        print(f\"NLTK resource '{resource_id}' (path: {resource_path}) not found. Attempting download...\")\n",
    "        try:\n",
    "            nltk.download(resource_id, quiet=False) # Download with quiet=False for more verbose output\n",
    "            print(f\"Successfully downloaded NLTK resource '{resource_id}'.\")\n",
    "            # Optionally, re-verify after download\n",
    "            nltk.data.find(resource_path)\n",
    "            print(f\"NLTK resource '{resource_id}' now available after download.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading NLTK resource '{resource_id}': {e}\")\n",
    "            print(f\"Please try manually downloading '{resource_id}' using 'nltk.download(\\\"{resource_id}\\\")' in a new cell if issues persist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking for NLTK resource '{resource_id}': {e}\")\n",
    "\n",
    "print(\"Finished NLTK resource checks.\")\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Set a plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Mount your Google Drive to Colab.\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False) # Set force_remount=True if you always want to re-authenticate\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "    # Define the output directory in Google Drive\n",
    "    # You can change 'Colab_Data_Collocations' to any folder name you prefer.\n",
    "    output_directory = '/content/drive/MyDrive/Colab_Data_Collocations_Pair'\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Output directory '{output_directory}' created successfully in your Google Drive.\")\n",
    "    else:\n",
    "        print(f\"Output directory '{output_directory}' already exists in your Google Drive.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Google Drive mounting or directory creation: {e}\")\n",
    "    output_directory = None # Ensure output_directory is None if mounting fails\n",
    "    print(\"Please check your Google Drive permissions and authentication.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Upload Your CSV File.\n",
    "# Import necessary library for file uploading in Colab\n",
    "from google.colab import files\n",
    "\n",
    "# Prompt user to upload a CSV file\n",
    "print('Please upload your CSV file:')\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check if a file was uploaded\n",
    "if uploaded:\n",
    "    # Get the name of the uploaded file\n",
    "    input_csv_name = list(uploaded.keys())[0]\n",
    "    print(f'Successfully uploaded \"{input_csv_name}\".')\n",
    "else:\n",
    "    input_csv_name = None\n",
    "    print('No file uploaded. Please upload a file to proceed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 5: Configure Parameters.\n",
    "#@markdown ### Input Data Configuration\n",
    "#@markdown Enter the name of the column in your CSV that contains the lemmatized text data.\n",
    "text_column_name = \"text_lemmatized\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown ### Target Word Configuration\n",
    "#@markdown Specify the two words you want to compare collocations for.\n",
    "word1_input = \"he\"  #@param {type:\"string\"}\n",
    "word2_input = \"she\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown ### Output File Configuration\n",
    "#@markdown Define the names for your output files. The plot will be a PNG image and the data will be a CSV file.\n",
    "output_plot_filename = \"collocation_plot.png\"  #@param {type:\"string\"}\n",
    "output_csv_filename = \"bigram_ratios.csv\"  #@param {type:\"string\"}\n",
    "\n",
    "# Basic validation for output file names\n",
    "if not output_plot_filename.lower().endswith('.png'):\n",
    "    output_plot_filename += '.png'\n",
    "    print(f\"Plot filename automatically appended with .png: {output_plot_filename}\")\n",
    "\n",
    "if not output_csv_filename.lower().endswith('.csv'):\n",
    "    output_csv_filename += '.csv'\n",
    "    print(f\"CSV filename automatically appended with .csv: {output_csv_filename}\")\n",
    "\n",
    "# Print the configured parameters to confirm choices\n",
    "print(\"--- Configuration Summary ---\")\n",
    "print(f\"Uploaded CSV file name: {input_csv_name if 'input_csv_name' in locals() else 'Not yet uploaded'}\")\n",
    "print(f\"Lemmatized text column: '{text_column_name}'\")\n",
    "print(f\"Word 1 for comparison: '{word1_input}'\")\n",
    "print(f\"Word 2 for comparison: '{word2_input}'\")\n",
    "print(f\"Output plot file name: '{output_plot_filename}'\")\n",
    "print(f\"Output CSV data file name: '{output_csv_filename}'\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Process Data, Calculate Ratios, and Generate Plot.\n",
    "print(\"Starting Step 6: Processing Data, Calculating Ratios...\")\n",
    "\n",
    "# Ensure input_csv_name is available from Step 4 and parameters from Step 5\n",
    "if 'input_csv_name' not in locals() or input_csv_name is None:\n",
    "    print(\"Error: CSV file name is not defined. Please upload a file in Step 4 and re-run this cell.\")\n",
    "elif 'text_column_name' not in locals() or 'word1_input' not in locals() or 'word2_input' not in locals():\n",
    "    print(\"Error: Configuration parameters (text_column_name, word1_input, word2_input) not defined. Please run Step 5 and re-run this cell.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the uploaded CSV file\n",
    "        df = pd.read_csv(input_csv_name)\n",
    "        print(f\"Successfully loaded '{input_csv_name}'.\")\n",
    "\n",
    "        # Check if the specified text column exists\n",
    "        if text_column_name not in df.columns:\n",
    "            print(f\"Error: Column '{text_column_name}' not found in the CSV. Available columns are: {df.columns.tolist()}. Please check the column name in Step 5.\")\n",
    "        else:\n",
    "            print(f\"Using column '{text_column_name}' for text processing.\")\n",
    "            # Handle potential empty strings or NaN values in the text column\n",
    "            df[text_column_name] = df[text_column_name].fillna('').astype(str)\n",
    "\n",
    "            # Prepare target words (pronouns), converting to lowercase\n",
    "            word1_processed = word1_input.lower()\n",
    "            word2_processed = word2_input.lower()\n",
    "            pronouns = [word1_processed, word2_processed]\n",
    "            print(f\"Target words for comparison (lowercase): {pronouns}\")\n",
    "\n",
    "            # --- Process Text and Generate Bigrams ---\n",
    "            all_bigrams = []\n",
    "            print(f\"Processing text from '{text_column_name}' to generate bigrams...\")\n",
    "            for text in df[text_column_name]:\n",
    "                # Ensure NLTK tokenizers are available (checked in Step 2, but good practice for standalone execution)\n",
    "                try:\n",
    "                    tokens = nltk.word_tokenize(text.lower()) # Tokenize and ensure text is lowercase\n",
    "                except Exception as e:\n",
    "                    print(f\"NLTK word_tokenize error: {e}. Ensure NLTK resources from Step 2 are downloaded.\")\n",
    "                    tokens = [] # Avoid further errors\n",
    "                current_bigrams = list(ngrams(tokens, 2)) # <--- CHANGED THIS LINE\n",
    "                all_bigrams.extend(current_bigrams)\n",
    "            \n",
    "            if not all_bigrams:\n",
    "                print(\"No bigrams were generated. This could be due to empty text in the specified column or issues with tokenization.\")\n",
    "                word_ratios_df = pd.DataFrame() # Ensure word_ratios_df exists\n",
    "            else:\n",
    "                print(f\"Generated a total of {len(all_bigrams)} bigrams.\")\n",
    "\n",
    "                # --- Count and Filter Bigrams ---\n",
    "                bigram_df = pd.DataFrame(all_bigrams, columns=['w1', 'w2'])\n",
    "                bigram_df['w1'] = bigram_df['w1'].str.lower()\n",
    "                bigram_df['w2'] = bigram_df['w2'].str.lower()\n",
    "\n",
    "                # Filter for bigrams starting with one of the target pronouns\n",
    "                filtered_bigrams = bigram_df[bigram_df['w1'].isin(pronouns)]\n",
    "\n",
    "                if filtered_bigrams.empty:\n",
    "                    print(f\"No bigrams found starting with the target words: {pronouns}. Cannot proceed with ratio calculation.\")\n",
    "                    word_ratios_df = pd.DataFrame() # Ensure word_ratios_df exists\n",
    "                else:\n",
    "                    print(f\"Found {len(filtered_bigrams)} bigrams starting with one of the target words.\")\n",
    "                    bigram_counts = filtered_bigrams.groupby(['w1', 'w2']).size().reset_index(name='n')\n",
    "\n",
    "                    # --- Calculate Word Ratios ---\n",
    "                    word_counts_pivot_prep = bigram_counts.rename(columns={'n': 'total'})\n",
    "\n",
    "                    # Filter out words (w2) where the sum of total counts (for that w2 across both pronouns) is not greater than 10\n",
    "                    w2_group_totals = word_counts_pivot_prep.groupby('w2')['total'].transform('sum')\n",
    "                    word_counts_filtered = word_counts_pivot_prep[w2_group_totals > 10]\n",
    "\n",
    "                    if word_counts_filtered.empty:\n",
    "                        print(\"No words (w2) met the frequency threshold (>10 total occurrences with target words). Cannot calculate ratios.\")\n",
    "                        word_ratios_df = pd.DataFrame() # Ensure word_ratios_df exists\n",
    "                    else:\n",
    "                        print(f\"Proceeding with {word_counts_filtered['w2'].nunique()} unique words (w2) that meet the frequency threshold.\")\n",
    "                        word_ratios_df = word_counts_filtered.pivot_table(\n",
    "                            index='w2',\n",
    "                            columns='w1',\n",
    "                            values='total',\n",
    "                            fill_value=0\n",
    "                        )\n",
    "\n",
    "                        if word1_processed not in word_ratios_df.columns:\n",
    "                            word_ratios_df[word1_processed] = 0\n",
    "                        if word2_processed not in word_ratios_df.columns:\n",
    "                            word_ratios_df[word2_processed] = 0\n",
    "                        \n",
    "                        word_ratios_df[word1_processed] = word_ratios_df[word1_processed] + 1\n",
    "                        word_ratios_df[word2_processed] = word_ratios_df[word2_processed] + 1\n",
    "\n",
    "                        col_sum1 = word_ratios_df[word1_processed].sum()\n",
    "                        col_sum2 = word_ratios_df[word2_processed].sum()\n",
    "                        \n",
    "                        word_ratios_df[f'{word1_processed}_norm'] = word_ratios_df[word1_processed] / col_sum1 if col_sum1 > 0 else 0\n",
    "                        word_ratios_df[f'{word2_processed}_norm'] = word_ratios_df[word2_processed] / col_sum2 if col_sum2 > 0 else 0\n",
    "\n",
    "                        epsilon = 1e-9 \n",
    "                        word_ratios_df['logratio'] = np.log2(\n",
    "                            (word_ratios_df[f'{word2_processed}_norm'] + epsilon) / \n",
    "                            (word_ratios_df[f'{word1_processed}_norm'] + epsilon)\n",
    "                        )\n",
    "\n",
    "                        word_ratios_df = word_ratios_df.sort_values(by='logratio', ascending=False)\n",
    "\n",
    "                        print(\"--- Word Ratios Dataframe (Top 5) ---\")\n",
    "                        print(word_ratios_df[['logratio', word1_processed, word2_processed, f'{word1_processed}_norm', f'{word2_processed}_norm']].head())\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_csv_name}' was not found. Please ensure it was uploaded correctly in Step 4 and re-run.\")\n",
    "        word_ratios_df = pd.DataFrame() \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data processing in Step 6: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        word_ratios_df = pd.DataFrame()\n",
    "\n",
    "print(\"Step 6 data processing finished.\")\n",
    "\n",
    "# --- Generate the Plot (appended to Step 6) ---\n",
    "if 'word_ratios_df' in locals() and not word_ratios_df.empty:\n",
    "    print(\"Preparing data for plotting...\")\n",
    "    plot_df = word_ratios_df.reset_index() \n",
    "    plot_df['abslogratio'] = np.abs(plot_df['logratio'])\n",
    "\n",
    "    N = 15\n",
    "    group_more_word1 = plot_df[plot_df['logratio'] < 0].nsmallest(N, 'logratio', keep='first') \n",
    "    group_more_word2 = plot_df[plot_df['logratio'] >= 0].nlargest(N, 'logratio', keep='first')\n",
    "\n",
    "    plot_data_df = pd.concat([group_more_word1, group_more_word2]).sort_values(by='logratio')\n",
    "\n",
    "    if plot_data_df.empty:\n",
    "        print(\"No data to plot after filtering top N words.\")\n",
    "    else:\n",
    "        print(f\"Plotting top {len(plot_data_df)} words by log ratio.\")\n",
    "        plt.figure(figsize=(10, max(6, len(plot_data_df) * 0.4)))\n",
    "\n",
    "        colors = ['#F8766D' if x < 0 else '#00BFC4' for x in plot_data_df['logratio']]\n",
    "\n",
    "        plt.hlines(y=plot_data_df['w2'], xmin=0, xmax=plot_data_df['logratio'], color=colors, alpha=0.8, linewidth=2)\n",
    "\n",
    "        plt.scatter(plot_data_df['logratio'], plot_data_df['w2'], color=colors, s=50, zorder=3)\n",
    "\n",
    "        plt.yticks(plot_data_df['w2'])\n",
    "        plt.ylabel(None)\n",
    "\n",
    "        plt.xlabel(f\"Log Ratio: More '{word1_processed}' <-> More '{word2_processed}'\")\n",
    "        x_ticks_values = np.array([-3, -2, -1, 0, 1, 2, 3])\n",
    "        x_ticks_labels = [f\"{2**abs(x):.0f}x\" if x != 0 else \"Same\" for x in x_ticks_values]\n",
    "        plt.xticks(x_ticks_values, x_ticks_labels)\n",
    "        plt.xlim(min(x_ticks_values)-0.5, max(x_ticks_values)+0.5) \n",
    "\n",
    "        plt.title(f\"Words More Associated with '{word1_processed.capitalize()}' vs '{word2_processed.capitalize()}'\", pad=20)\n",
    "\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='w', label=f'More \"{word2_processed.capitalize()}\"', markerfacecolor='#00BFC4', markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label=f'More \"{word1_processed.capitalize()}\"', markerfacecolor='#F8766D', markersize=10)\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.15 - (0.03 * (len(plot_data_df)/10) ) ) )\n",
    "\n",
    "        plt.grid(True, which='major', axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.grid(False, which='major', axis='y') \n",
    "        sns.despine(left=True, bottom=True) \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 0.95]) \n",
    "        \n",
    "        temp_plot_path = \"/tmp/plot_for_download.png\"\n",
    "        try:\n",
    "            plt.savefig(temp_plot_path, bbox_inches='tight')\n",
    "            print(f\"Plot temporarily saved to {temp_plot_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot to temporary file: {e}\")\n",
    "            temp_plot_path = None \n",
    "\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Skipping plot generation as 'word_ratios_df' is empty or not defined.\")\n",
    "\n",
    "print(\"Step 6 (including plotting) execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Save Plot and Data, and Provide Download Links.\n",
    "print(\"Starting Step 7: Saving results and providing download links...\")\n",
    "\n",
    "if 'output_directory' not in locals() or not output_directory:\n",
    "    print(\"Error: Google Drive output directory ('output_directory') is not defined. Please run Step 3 (Mount Google Drive).\")\n",
    "else:\n",
    "    if 'temp_plot_path' in locals() and temp_plot_path and os.path.exists(temp_plot_path):\n",
    "        try:\n",
    "            output_plot_drive_path = os.path.join(output_directory, output_plot_filename)\n",
    "            shutil.copy(temp_plot_path, output_plot_drive_path)\n",
    "            print(f\"Plot successfully saved to Google Drive: {output_plot_drive_path}\")\n",
    "            print(f\"Offering plot image '{output_plot_filename}' for download...\")\n",
    "            files.download(output_plot_drive_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying plot to Google Drive or providing download: {e}\")\n",
    "    elif 'temp_plot_path' in locals() and temp_plot_path and not os.path.exists(temp_plot_path):\n",
    "        print(f\"Error: Temporary plot file {temp_plot_path} not found. Plot might not have been saved correctly in Step 6.\")\n",
    "    else:\n",
    "        print(\"Skipping plot saving/downloading as the plot was not generated or saved in Step 6.\")\n",
    "\n",
    "    if 'word_ratios_df' in locals() and not word_ratios_df.empty:\n",
    "        try:\n",
    "            output_csv_drive_path = os.path.join(output_directory, output_csv_filename)\n",
    "            df_to_save = word_ratios_df.reset_index()\n",
    "            df_to_save.to_csv(output_csv_drive_path, index=False)\n",
    "            print(f\"Data CSV successfully saved to Google Drive: {output_csv_drive_path}\")\n",
    "            print(f\"Offering data CSV '{output_csv_filename}' for download...\")\n",
    "            files.download(output_csv_drive_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data CSV to Google Drive or providing download: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping data CSV saving/downloading as 'word_ratios_df' is empty or not defined from Step 6.\")\n",
    "\n",
    "print(\"Step 7 execution finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Analysis Complete!\n",
    "\n",
    "Thank you for using the **Plot Collocations Pair Tool**!\n",
    "\n",
    "## Summary of Results:\n",
    "The analysis has finished. If all steps were successful:\n",
    "*   Your collocation plot (e.g., `collocation_plot.png`) and the associated data CSV (e.g., `bigram_ratios.csv`) have been saved to your Google Drive in the directory specified in Step 3 (typically `MyDrive/Colab_Data_Collocations_Pair`).\n",
    "*   Download links for these files were also provided directly in Step 7.\n",
    "\n",
    "## Troubleshooting Common Issues:\n",
    "\n",
    "If you encountered any problems, here are some common troubleshooting tips:\n",
    "\n",
    "1.  **File Not Found Errors**:\n",
    "    *   **CSV Upload (Step 4)**: Ensure you uploaded your CSV file successfully. If you re-run the notebook, you might need to re-upload.\n",
    "    *   **Google Drive Path (Step 3 & 7)**: Verify that Google Drive was mounted correctly and the `output_directory` path is valid. Check for typos.\n",
    "    *   **Temporary Plot File (Step 7)**: If you see errors about `/tmp/plot_for_download.png` not found, it means Step 6 failed to save the plot image before Step 7 tried to access it. Review errors in Step 6.\n",
    "\n",
    "2.  **Incorrect Text Column (Step 5)**:\n",
    "    *   Double-check that the `text_column_name` you specified in Step 5 exactly matches the column header in your CSV file that contains the lemmatized text.\n",
    "\n",
    "3.  **CSV Encoding Issues**:\n",
    "    *   If you see errors when pandas tries to read the CSV (`pd.read_csv()` in Step 6), your file might have a non-standard encoding. Try specifying the encoding, e.g., `pd.read_csv(input_csv_name, encoding='latin1')` or `encoding='iso-8859-1'`. UTF-8 is the default and usually preferred.\n",
    "\n",
    "4.  **Google Drive Permissions/Quota (Step 3 & 7)**:\n",
    "    *   Ensure Colab has the necessary permissions to access your Google Drive.\n",
    "    *   Check if your Google Drive has sufficient storage space.\n",
    "    *   Sometimes, re-mounting Google Drive (by re-running Step 3, potentially with `force_remount=True`) can resolve transient issues.\n",
    "\n",
    "5.  **Handling of Large Files / Colab Limits**:\n",
    "    *   Processing very large text corpora can be time-consuming and memory-intensive. Colab has usage limits. If the notebook crashes or is very slow, consider:\n",
    "        *   Using a smaller subset of your data for testing.\n",
    "        *   Optimizing data loading and processing if possible (though the current script is reasonably standard).\n",
    "        *   Using a more powerful environment if your task consistently exceeds Colab's capabilities.\n",
    "\n",
    "6.  **Plot Not Looking as Expected (Step 6 Output)**:\n",
    "    *   **Input Words (Step 5)**: Ensure the `word1_input` and `word2_input` are correctly specified and are present in your data.\n",
    "    *   **Data Quality**: The quality of the plot depends heavily on the input text data (e.g., quality of lemmatization, size of corpus).\n",
    "    *   **Filtering Thresholds (Step 6)**: The script filters for words (collocates) that appear more than 10 times with the target words. If your dataset is small or your target words are infrequent, you might get few or no results. Consider adjusting this threshold in the code of Step 6 if necessary (look for `w2_group_totals > 10`).\n",
    "    *   **Zero Counts / Errors**: If your chosen words (e.g., 'he', 'she') or their collocates are very infrequent or absent in the provided text column, you might see empty plots, zero log ratios, or errors. Check the console output in Step 6 for messages about counts.\n",
    "\n",
    "7.  **NLTK Resource Download (Step 2)**:\n",
    "    *   The notebook attempts to download the 'punkt' tokenizer models for NLTK if they're not found. If this fails, it's usually due to network issues. Try running Step 2 again.\n",
    "\n",
    "If issues persist, carefully review the error messages in each cell's output. These messages often provide specific clues about what went wrong.\n",
    "\n",
    "Happy analyzing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
